{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "9a1aa9f2cc29473f9f8e5459d2641e76",
      "308fa6a7348140ec981a8d6c7d31f346",
      "8bc92587e35443488445e7521fbd0a13",
      "091f8220f33241f288faa0612853585f",
      "1045bb16e3694410898a73cf1b848917",
      "cb95e545fbdd4e99903bf634df694c9f",
      "5080d322a8034924b652b379c04667ed",
      "8b1899a0c4b144d7a5e6599f8afb8b65",
      "6111a73e684a47769bda7183a836ee91",
      "cd3570ddf67541d7818d97e236c54e54",
      "bbba60f793c14100934a268063f63d26"
     ]
    },
    "id": "sd1LiXGjZ420",
    "outputId": "1b5783cd-0e4e-4c92-c67c-57b9288f2381"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dell/anaconda3/envs/fire/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 100%|██████████| 3.03k/3.03k [00:00<00:00, 6.24MB/s]\n",
      "Downloading data: 100%|██████████████████████| 880k/880k [00:00<00:00, 2.28MB/s]\n",
      "Generating train split: 100%|█████| 7936/7936 [00:00<00:00, 29062.85 examples/s]\n",
      "Generating test split: 100%|██████| 1985/1985 [00:00<00:00, 29316.44 examples/s]\n",
      "Generating validation split: 100%|█| 1985/1985 [00:00<00:00, 29144.65 examples/s\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Siddharthkak/TWEETS\", download_mode='force_redownload')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pRd1kXQZjYIY",
    "outputId": "8021590c-5607-4a9c-a474-bc57da503c93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ID', 'tweet', 'unnecessary', 'mandatory', 'pharma', 'conspiracy', 'political', 'country', 'rushed', 'ingredients', 'side-effect', 'ineffective', 'religious', 'none'],\n",
       "        num_rows: 7936\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ID', 'tweet', 'unnecessary', 'mandatory', 'pharma', 'conspiracy', 'political', 'country', 'rushed', 'ingredients', 'side-effect', 'ineffective', 'religious', 'none'],\n",
       "        num_rows: 1985\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ID', 'tweet', 'unnecessary', 'mandatory', 'pharma', 'conspiracy', 'political', 'country', 'rushed', 'ingredients', 'side-effect', 'ineffective', 'religious', 'none'],\n",
       "        num_rows: 1985\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "unjuTtKUjZI3",
    "outputId": "6f1e5051-8272-40f9-ced8-ba17a105e904"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': '1352994869556350977t',\n",
       " 'tweet': ' trustccp  do you trust the chinese communist party to make your mrna  vaccine   well  good  because there are hundreds of their virologists working hard for you right now in big pharma    ',\n",
       " 'unnecessary': False,\n",
       " 'mandatory': False,\n",
       " 'pharma': False,\n",
       " 'conspiracy': False,\n",
       " 'political': False,\n",
       " 'country': True,\n",
       " 'rushed': False,\n",
       " 'ingredients': False,\n",
       " 'side-effect': False,\n",
       " 'ineffective': False,\n",
       " 'religious': False,\n",
       " 'none': False}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset['train'][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e5vZhQpvkE8s",
    "outputId": "5d513b30-f209-492f-c6ab-245d64a67d40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unnecessary',\n",
       " 'mandatory',\n",
       " 'pharma',\n",
       " 'conspiracy',\n",
       " 'political',\n",
       " 'country',\n",
       " 'rushed',\n",
       " 'ingredients',\n",
       " 'side-effect',\n",
       " 'ineffective',\n",
       " 'religious',\n",
       " 'none']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label for label in dataset['train'].features.keys() if label not in ['ID', 'tweet']]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJ3Teyjmank2"
   },
   "source": [
    "## Preprocess data\n",
    "\n",
    "As models like BERT don't expect text as direct input, but rather `input_ids`, etc., we tokenize the text using the tokenizer. Here I'm using the `AutoTokenizer` API, which will automatically load the appropriate tokenizer based on the checkpoint on the hub.\n",
    "\n",
    "What's a bit tricky is that we also need to provide labels to the model. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). Also important: this should be a tensor of floats rather than integers, otherwise PyTorch' `BCEWithLogitsLoss` (which the model will use) will complain, as explained [here](https://discuss.pytorch.org/t/multi-label-binary-classification-result-type-float-cant-be-cast-to-the-desired-output-type-long/117915/3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AFWlSsbZaRLc"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"digitalepidemiologylab/covid-twitter-bert-v2\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "  # take a batch of texts\n",
    "  text = examples[\"tweet\"]\n",
    "  # encode them\n",
    "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "  # add labels\n",
    "  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "  # create numpy array of shape (batch_size, num_labels)\n",
    "  labels_matrix = np.zeros((len(text), len(labels)))\n",
    "  # fill numpy array\n",
    "  for idx, label in enumerate(labels):\n",
    "    labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "  encoding[\"labels\"] = labels_matrix.tolist()\n",
    "\n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4ENBTdulBEI",
    "outputId": "02554a1f-4961-461a-bf29-555b8debeabf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████| 7936/7936 [00:00<00:00, 24677.39 examples/s]\n",
      "Map: 100%|████████████████████████| 1985/1985 [00:00<00:00, 24956.82 examples/s]\n",
      "Map: 100%|████████████████████████| 1985/1985 [00:00<00:00, 27405.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0enAb0W9o25W",
    "outputId": "55bc5ba6-d169-49c6-f562-bb7ea4143866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "example = encoded_dataset['train'][0]\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "D0McCtJ8HRJY",
    "outputId": "82fb0336-51a3-40ad-ebc0-65eeb7cf4b6c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] trustccp do you trust the chinese communist party to make your mrna vaccine well good because there are hundreds of their virologists working hard for you right now in big pharma [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VdIvj6WjHeZQ",
    "outputId": "418c14d2-cca3-44e9-d0a2-6ad4ca7a007c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4Dx95t2o6N9",
    "outputId": "3ce6c923-0b45-4743-bc4b-7771d088b03e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['country']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgpKXDfvKBxn"
   },
   "source": [
    "Finally, we set the format of our data to PyTorch tensors. This will turn the training, validation and test sets into standard PyTorch [datasets](https://pytorch.org/docs/stable/data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Lk6Cq9duKBkA"
   },
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5qSmCgWefWs"
   },
   "source": [
    "## Define model\n",
    "\n",
    "Here we define a model that includes a pre-trained base (i.e. the weights from bert-base-uncased) are loaded, with a random initialized classification head (linear layer) on top. One should fine-tune this head, together with the pre-trained base on a labeled dataset.\n",
    "\n",
    "This is also printed by the warning.\n",
    "\n",
    "We set the `problem_type` to be \"multi_label_classification\", as this will make sure the appropriate loss function is used (namely [`BCEWithLogitsLoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)). We also make sure the output layer has `len(labels)` output neurons, and we set the id2label and label2id mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XPL1Z_RegBF",
    "outputId": "22994300-8c93-421e-faa4-678d6cc14aab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at digitalepidemiologylab/covid-twitter-bert-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"digitalepidemiologylab/covid-twitter-bert-v2\",\n",
    "                                                           problem_type=\"multi_label_classification\",\n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjJGEXShp7te"
   },
   "source": [
    "## Train the model!\n",
    "\n",
    "We are going to train the model using HuggingFace's Trainer API. This requires us to define 2 things:\n",
    "\n",
    "* `TrainingArguments`, which specify training hyperparameters. All options can be found in the [docs](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments). Below, we for example specify that we want to evaluate after every epoch of training, we would like to save the model every epoch, we set the learning rate, the batch size to use for training/evaluation, how many epochs to train for, and so on.\n",
    "* a `Trainer` object (docs can be found [here](https://huggingface.co/transformers/main_classes/trainer.html#id1))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "K5a8_vIKqr7P"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "metric_name = \"f1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "dR2GmpvDqbuZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"covid-twitter-bert-v2-fire\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_v2fPFFJ3-v"
   },
   "source": [
    "We are also going to compute metrics while training. For this, we need to define a `compute_metrics` function, that returns a dictionary with the desired metric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "797b2WHJqUgZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "\n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions,\n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds,\n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxNo4_TsvzDm"
   },
   "source": [
    "Let's verify a batch as well as a forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "IlOgGiojuWwG",
    "outputId": "cd0b2c99-b520-468d-8ffc-c36211b7820a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train'][0]['labels'].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y41Kre_jvD7x",
    "outputId": "b6ca888b-6371-40fb-ab83-3dc24d28320a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  3404,  9468,  2361,  2079,  2017,  3404,  1996,  2822,  4750,\n",
       "         2283,  2000,  2191,  2115, 28848, 17404,  2092,  2204,  2138,  2045,\n",
       "         2024,  5606,  1997,  2037,  6819, 13153, 22522,  2015,  2551,  2524,\n",
       "         2005,  2017,  2157,  2085,  1999,  2502,  6887, 27292,  2050,   102,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train']['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sxWcnZ8ku12V",
    "outputId": "26522911-c3cd-466a-ae2d-d81d23003c23"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.7630, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), logits=tensor([[-1.2400,  1.4422,  0.8697, -0.0482,  0.2585,  0.6595, -2.3150, -0.2095,\n",
       "         -0.5722,  0.4826,  1.1486, -0.2237]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#forward pass\n",
    "outputs = model(input_ids=encoded_dataset['train']['input_ids'][0].unsqueeze(0), labels=encoded_dataset['train'][0]['labels'].unsqueeze(0))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-X2brZcv0X6"
   },
   "source": [
    "Let's start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "chq_3nUz73ib"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KXmFds8js6P8",
    "outputId": "66ebb2ab-f93f-48aa-a4dc-36f55f2f8559"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dell/anaconda3/envs/fire/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14880' max='14880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14880/14880 45:04, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.255700</td>\n",
       "      <td>0.148713</td>\n",
       "      <td>0.703880</td>\n",
       "      <td>0.806079</td>\n",
       "      <td>0.566247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.124100</td>\n",
       "      <td>0.134998</td>\n",
       "      <td>0.744033</td>\n",
       "      <td>0.835892</td>\n",
       "      <td>0.610579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.084700</td>\n",
       "      <td>0.142140</td>\n",
       "      <td>0.765514</td>\n",
       "      <td>0.861909</td>\n",
       "      <td>0.631738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>0.155341</td>\n",
       "      <td>0.751426</td>\n",
       "      <td>0.850099</td>\n",
       "      <td>0.620151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.039100</td>\n",
       "      <td>0.173441</td>\n",
       "      <td>0.753283</td>\n",
       "      <td>0.864363</td>\n",
       "      <td>0.610076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>0.182904</td>\n",
       "      <td>0.753773</td>\n",
       "      <td>0.870036</td>\n",
       "      <td>0.607053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>0.200939</td>\n",
       "      <td>0.755906</td>\n",
       "      <td>0.873994</td>\n",
       "      <td>0.603023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.203048</td>\n",
       "      <td>0.762915</td>\n",
       "      <td>0.872606</td>\n",
       "      <td>0.621159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.213866</td>\n",
       "      <td>0.764612</td>\n",
       "      <td>0.875845</td>\n",
       "      <td>0.620151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.219420</td>\n",
       "      <td>0.764928</td>\n",
       "      <td>0.876721</td>\n",
       "      <td>0.617632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.231764</td>\n",
       "      <td>0.759415</td>\n",
       "      <td>0.878836</td>\n",
       "      <td>0.608564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.231056</td>\n",
       "      <td>0.761980</td>\n",
       "      <td>0.875603</td>\n",
       "      <td>0.610076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.236380</td>\n",
       "      <td>0.759754</td>\n",
       "      <td>0.874266</td>\n",
       "      <td>0.613098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.238080</td>\n",
       "      <td>0.760518</td>\n",
       "      <td>0.872568</td>\n",
       "      <td>0.616121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.239650</td>\n",
       "      <td>0.763456</td>\n",
       "      <td>0.875501</td>\n",
       "      <td>0.618136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=14880, training_loss=0.03976519100608364, metrics={'train_runtime': 2704.9953, 'train_samples_per_second': 44.007, 'train_steps_per_second': 5.501, 'total_flos': 2.773521445552128e+16, 'train_loss': 0.03976519100608364, 'epoch': 15.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiloh9eMK91o"
   },
   "source": [
    "## Evaluate\n",
    "\n",
    "After training, we evaluate our model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "cMlebJ83LRYG",
    "outputId": "b18102e7-2198-4beb-c874-d39636f740ed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='249' max='249' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [249/249 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.14214031398296356,\n",
       " 'eval_f1': 0.7655143690129114,\n",
       " 'eval_roc_auc': 0.8619085505449334,\n",
       " 'eval_accuracy': 0.6317380352644836,\n",
       " 'eval_runtime': 11.2074,\n",
       " 'eval_samples_per_second': 177.115,\n",
       " 'eval_steps_per_second': 22.217,\n",
       " 'epoch': 15.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nmvJp0pLq-3"
   },
   "source": [
    "## Inference\n",
    "\n",
    "Let's test the model on a new sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1070378532260470789t</td>\n",
       "      <td>study links hpv vaccine to historically high i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>973746711964372993t</td>\n",
       "      <td>deaths from tainted measles vaccine affecting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1043031076787040257t</td>\n",
       "      <td>am apreciat un videoclip pe  treatment and cur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1066338147527741440t</td>\n",
       "      <td>video    gt  gt  mmr  vaccine increase risk of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>963522018544152576t</td>\n",
       "      <td>oral polio vaccine  infecting unvaccinated kid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>1099937642169405440t</td>\n",
       "      <td>i know it is hard to believe    but another va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>1536674508731518985t</td>\n",
       "      <td>seizures on the very day of the mmr vaccine  t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>1183314494874968064t</td>\n",
       "      <td>kenyan doctors say unicef is making women barr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>1327581896243556352t</td>\n",
       "      <td>not doing it  my experience with vaccines  in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>1407551365669265408t</td>\n",
       "      <td>morgan stanley plans to block unvaccinated emp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>486 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ID                                              tweet\n",
       "0    1070378532260470789t  study links hpv vaccine to historically high i...\n",
       "1     973746711964372993t  deaths from tainted measles vaccine affecting ...\n",
       "2    1043031076787040257t  am apreciat un videoclip pe  treatment and cur...\n",
       "3    1066338147527741440t  video    gt  gt  mmr  vaccine increase risk of...\n",
       "4     963522018544152576t  oral polio vaccine  infecting unvaccinated kid...\n",
       "..                    ...                                                ...\n",
       "481  1099937642169405440t  i know it is hard to believe    but another va...\n",
       "482  1536674508731518985t  seizures on the very day of the mmr vaccine  t...\n",
       "483  1183314494874968064t  kenyan doctors say unicef is making women barr...\n",
       "484  1327581896243556352t  not doing it  my experience with vaccines  in ...\n",
       "485  1407551365669265408t  morgan stanley plans to block unvaccinated emp...\n",
       "\n",
       "[486 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['side-effect']\n",
      "['ingredients', 'side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['none']\n",
      "['ineffective']\n",
      "['ingredients', 'side-effect']\n",
      "['pharma', 'side-effect']\n",
      "['ingredients', 'side-effect']\n",
      "['ingredients']\n",
      "['none']\n",
      "['side-effect', 'ineffective']\n",
      "['side-effect']\n",
      "['none']\n",
      "['ineffective']\n",
      "['side-effect', 'ineffective']\n",
      "['ineffective']\n",
      "[]\n",
      "['ineffective']\n",
      "['side-effect', 'ineffective']\n",
      "[]\n",
      "['side-effect', 'ineffective']\n",
      "['ineffective']\n",
      "['ineffective']\n",
      "['pharma', 'ineffective']\n",
      "['pharma']\n",
      "['pharma']\n",
      "['pharma']\n",
      "['ineffective']\n",
      "['pharma', 'side-effect']\n",
      "['pharma', 'side-effect']\n",
      "['pharma']\n",
      "['pharma']\n",
      "['pharma', 'side-effect']\n",
      "['none']\n",
      "['none']\n",
      "['rushed']\n",
      "['none']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['ineffective']\n",
      "['unnecessary', 'side-effect']\n",
      "['none']\n",
      "['ineffective']\n",
      "['mandatory']\n",
      "['mandatory']\n",
      "['mandatory']\n",
      "['mandatory', 'side-effect']\n",
      "['mandatory']\n",
      "['mandatory', 'side-effect']\n",
      "['mandatory']\n",
      "['mandatory', 'pharma', 'political']\n",
      "['mandatory', 'side-effect']\n",
      "['mandatory', 'side-effect']\n",
      "['conspiracy']\n",
      "['pharma', 'conspiracy']\n",
      "['pharma']\n",
      "['conspiracy']\n",
      "['conspiracy']\n",
      "['ineffective']\n",
      "['pharma']\n",
      "['conspiracy']\n",
      "['side-effect']\n",
      "['pharma']\n",
      "['ingredients']\n",
      "['ingredients', 'side-effect']\n",
      "['ingredients', 'side-effect']\n",
      "['ingredients']\n",
      "['ingredients', 'side-effect']\n",
      "['ingredients', 'side-effect']\n",
      "['side-effect']\n",
      "['ingredients']\n",
      "['ingredients', 'side-effect']\n",
      "['ingredients']\n",
      "['rushed', 'side-effect']\n",
      "['rushed', 'side-effect']\n",
      "['rushed', 'ingredients', 'side-effect']\n",
      "['pharma', 'ineffective']\n",
      "['rushed', 'side-effect']\n",
      "['rushed']\n",
      "['rushed', 'side-effect']\n",
      "['rushed', 'side-effect']\n",
      "['side-effect']\n",
      "['pharma', 'political']\n",
      "['side-effect']\n",
      "['political']\n",
      "['pharma']\n",
      "['pharma', 'political']\n",
      "['political', 'side-effect']\n",
      "['pharma']\n",
      "['political', 'side-effect']\n",
      "['political']\n",
      "[]\n",
      "['ingredients', 'religious']\n",
      "['ingredients', 'religious']\n",
      "['side-effect']\n",
      "[]\n",
      "['ingredients', 'side-effect']\n",
      "['religious']\n",
      "[]\n",
      "['religious']\n",
      "['ingredients', 'religious']\n",
      "['side-effect']\n",
      "[]\n",
      "['pharma']\n",
      "['country']\n",
      "[]\n",
      "['side-effect']\n",
      "['country']\n",
      "['country']\n",
      "['country']\n",
      "['pharma']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['ingredients', 'side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['mandatory', 'pharma']\n",
      "['mandatory']\n",
      "['mandatory']\n",
      "['mandatory']\n",
      "['mandatory']\n",
      "['side-effect']\n",
      "['mandatory', 'conspiracy']\n",
      "[]\n",
      "['pharma']\n",
      "['rushed']\n",
      "['conspiracy']\n",
      "['political']\n",
      "['mandatory', 'political']\n",
      "['pharma']\n",
      "['political']\n",
      "['pharma']\n",
      "['political']\n",
      "['mandatory']\n",
      "['political']\n",
      "['conspiracy', 'political']\n",
      "['ineffective']\n",
      "['side-effect']\n",
      "['side-effect', 'ineffective']\n",
      "['side-effect', 'ineffective']\n",
      "['side-effect']\n",
      "['ineffective']\n",
      "['side-effect']\n",
      "['ineffective']\n",
      "['side-effect', 'ineffective']\n",
      "['mandatory']\n",
      "[]\n",
      "['mandatory']\n",
      "['none']\n",
      "['none']\n",
      "['mandatory']\n",
      "['ingredients']\n",
      "[]\n",
      "['side-effect', 'ineffective']\n",
      "['political', 'side-effect']\n",
      "['rushed']\n",
      "['political', 'rushed']\n",
      "['rushed']\n",
      "['rushed']\n",
      "['rushed', 'side-effect']\n",
      "['rushed', 'side-effect']\n",
      "['rushed', 'side-effect']\n",
      "['rushed']\n",
      "['rushed', 'side-effect']\n",
      "['rushed', 'side-effect']\n",
      "['unnecessary']\n",
      "['rushed']\n",
      "['ineffective']\n",
      "[]\n",
      "['none']\n",
      "['none']\n",
      "['side-effect']\n",
      "[]\n",
      "['side-effect', 'ineffective']\n",
      "['unnecessary']\n",
      "['conspiracy', 'side-effect']\n",
      "['conspiracy']\n",
      "['pharma', 'side-effect']\n",
      "['conspiracy', 'ingredients', 'side-effect']\n",
      "['conspiracy', 'side-effect']\n",
      "['side-effect']\n",
      "['conspiracy', 'side-effect']\n",
      "['conspiracy']\n",
      "['pharma']\n",
      "['conspiracy', 'side-effect']\n",
      "['ingredients']\n",
      "['ingredients', 'side-effect']\n",
      "['ingredients', 'side-effect']\n",
      "['ingredients']\n",
      "['ingredients', 'side-effect']\n",
      "['ingredients']\n",
      "['ingredients']\n",
      "['ingredients', 'side-effect']\n",
      "['ingredients', 'side-effect']\n",
      "['ingredients', 'side-effect']\n",
      "['conspiracy']\n",
      "['pharma', 'rushed']\n",
      "['pharma']\n",
      "['pharma', 'side-effect']\n",
      "['pharma', 'side-effect']\n",
      "['pharma']\n",
      "['pharma', 'political']\n",
      "['pharma', 'rushed']\n",
      "['pharma']\n",
      "['pharma']\n",
      "['political']\n",
      "['religious']\n",
      "[]\n",
      "['religious']\n",
      "[]\n",
      "['ingredients']\n",
      "['ingredients', 'religious']\n",
      "['religious']\n",
      "['ineffective']\n",
      "[]\n",
      "['country']\n",
      "['pharma']\n",
      "['political']\n",
      "['country']\n",
      "[]\n",
      "['rushed']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['ineffective']\n",
      "['ineffective']\n",
      "['ineffective']\n",
      "['ineffective']\n",
      "['ineffective']\n",
      "['side-effect', 'ineffective']\n",
      "['ineffective']\n",
      "['ineffective']\n",
      "['side-effect']\n",
      "['side-effect', 'ineffective']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['rushed', 'side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['political', 'side-effect']\n",
      "['mandatory']\n",
      "['mandatory', 'side-effect']\n",
      "['mandatory', 'side-effect']\n",
      "['pharma']\n",
      "['mandatory']\n",
      "[]\n",
      "['mandatory']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['ingredients', 'side-effect']\n",
      "['ingredients']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['none']\n",
      "['none']\n",
      "['side-effect']\n",
      "[]\n",
      "['mandatory']\n",
      "[]\n",
      "['unnecessary']\n",
      "['ingredients', 'side-effect']\n",
      "['side-effect']\n",
      "['none']\n",
      "['side-effect']\n",
      "['none']\n",
      "['unnecessary']\n",
      "['conspiracy']\n",
      "['pharma', 'side-effect']\n",
      "['mandatory']\n",
      "['pharma', 'ineffective']\n",
      "['pharma']\n",
      "['pharma']\n",
      "['pharma']\n",
      "['side-effect']\n",
      "['pharma']\n",
      "['pharma']\n",
      "['conspiracy', 'side-effect']\n",
      "['conspiracy', 'ingredients', 'side-effect']\n",
      "['side-effect']\n",
      "['conspiracy', 'side-effect']\n",
      "['pharma', 'side-effect']\n",
      "['side-effect']\n",
      "['pharma']\n",
      "['conspiracy']\n",
      "[]\n",
      "['conspiracy']\n",
      "['ingredients', 'side-effect']\n",
      "['ingredients']\n",
      "['ingredients']\n",
      "['ingredients']\n",
      "['ingredients']\n",
      "['ingredients']\n",
      "['ingredients']\n",
      "['ingredients', 'side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['rushed']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['rushed']\n",
      "['rushed']\n",
      "['rushed', 'side-effect']\n",
      "['rushed']\n",
      "['rushed', 'side-effect']\n",
      "['rushed', 'side-effect']\n",
      "['pharma']\n",
      "['side-effect']\n",
      "['political']\n",
      "['political']\n",
      "['political', 'side-effect']\n",
      "['political']\n",
      "['political', 'ineffective']\n",
      "['political', 'side-effect']\n",
      "['political', 'side-effect']\n",
      "['political']\n",
      "['pharma']\n",
      "['political']\n",
      "['side-effect']\n",
      "['pharma']\n",
      "['religious']\n",
      "['mandatory']\n",
      "['side-effect']\n",
      "['conspiracy', 'religious']\n",
      "['conspiracy']\n",
      "['mandatory']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['ingredients']\n",
      "[]\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['mandatory']\n",
      "['rushed']\n",
      "['ineffective']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect', 'ineffective']\n",
      "['side-effect']\n",
      "['mandatory', 'pharma']\n",
      "['mandatory', 'side-effect']\n",
      "['mandatory', 'side-effect']\n",
      "['side-effect']\n",
      "['pharma', 'side-effect']\n",
      "['unnecessary']\n",
      "['side-effect']\n",
      "['mandatory']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['ingredients', 'side-effect']\n",
      "['side-effect']\n",
      "['pharma', 'political']\n",
      "['political', 'side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['mandatory', 'political']\n",
      "['side-effect']\n",
      "['pharma']\n",
      "['side-effect']\n",
      "[]\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['rushed', 'side-effect']\n",
      "['side-effect']\n",
      "[]\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['mandatory']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['pharma']\n",
      "['side-effect']\n",
      "['none']\n",
      "['ingredients']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['mandatory']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['mandatory']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['pharma', 'ineffective']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['pharma']\n",
      "['side-effect']\n",
      "['mandatory']\n",
      "['ineffective']\n",
      "['unnecessary', 'side-effect']\n",
      "['side-effect']\n",
      "['mandatory']\n",
      "['ingredients', 'side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['rushed']\n",
      "['pharma']\n",
      "[]\n",
      "['ineffective']\n",
      "[]\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect', 'ineffective']\n",
      "['side-effect']\n",
      "['none']\n",
      "['side-effect']\n",
      "['ineffective']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['ineffective']\n",
      "['pharma']\n",
      "['ineffective']\n",
      "['ingredients']\n",
      "['side-effect', 'ineffective']\n",
      "['side-effect']\n",
      "['unnecessary']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['conspiracy']\n",
      "['mandatory']\n",
      "['side-effect']\n",
      "['pharma', 'ineffective']\n",
      "['pharma', 'side-effect']\n",
      "['side-effect']\n",
      "['none']\n",
      "['side-effect']\n",
      "['ineffective']\n",
      "['rushed']\n",
      "['conspiracy', 'side-effect']\n",
      "['mandatory', 'side-effect']\n",
      "['unnecessary']\n",
      "['side-effect']\n",
      "['ineffective']\n",
      "['ingredients']\n",
      "['pharma', 'side-effect']\n",
      "['side-effect']\n",
      "['political']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['pharma']\n",
      "['ineffective']\n",
      "['side-effect']\n",
      "['conspiracy', 'ingredients']\n",
      "['ingredients']\n",
      "['side-effect']\n",
      "['none']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n",
      "['side-effect']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "    # id = data['ID'].iloc[i]\n",
    "    # data['id'].iloc[i] = str(id)\n",
    "    text = data['tweet'].iloc[i]\n",
    "    encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "    encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
    "    outputs = trainer.model(**encoding)\n",
    "    logits = outputs.logits\n",
    "    # apply sigmoid + threshold\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(logits.squeeze().cpu())\n",
    "    predictions = np.zeros(probs.shape)\n",
    "    predictions[np.where(probs >= 0.5)] = 1\n",
    "    # turn predicted id's into actual label names\n",
    "    predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "    print(predicted_labels)\n",
    "    if len(predicted_labels) < 1:\n",
    "        data['tweet'].iloc[i] = \"none\"\n",
    "    else:\n",
    "        data['tweet'].iloc[i] = \" \".join(predicted_labels)\n",
    "    \n",
    "data.rename(columns = {'ID':'id', 'tweet':'pred_classes'}, inplace = True)\n",
    "# data = data.drop(['tweet', 'ID' ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pred_classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1070378532260470789t</td>\n",
       "      <td>side-effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>973746711964372993t</td>\n",
       "      <td>ingredients side-effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1043031076787040257t</td>\n",
       "      <td>side-effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1066338147527741440t</td>\n",
       "      <td>side-effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>963522018544152576t</td>\n",
       "      <td>side-effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>1099937642169405440t</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>1536674508731518985t</td>\n",
       "      <td>side-effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>1183314494874968064t</td>\n",
       "      <td>side-effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>1327581896243556352t</td>\n",
       "      <td>side-effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>1407551365669265408t</td>\n",
       "      <td>side-effect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>486 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id             pred_classes\n",
       "0    1070378532260470789t              side-effect\n",
       "1     973746711964372993t  ingredients side-effect\n",
       "2    1043031076787040257t              side-effect\n",
       "3    1066338147527741440t              side-effect\n",
       "4     963522018544152576t              side-effect\n",
       "..                    ...                      ...\n",
       "481  1099937642169405440t                     none\n",
       "482  1536674508731518985t              side-effect\n",
       "483  1183314494874968064t              side-effect\n",
       "484  1327581896243556352t              side-effect\n",
       "485  1407551365669265408t              side-effect\n",
       "\n",
       "[486 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('run1_covid_twitter_bert.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "3fxjfr8PLD42"
   },
   "outputs": [],
   "source": [
    "text = data['tweet'].iloc[0]\n",
    "\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
    "\n",
    "outputs = trainer.model(**encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8THm5-XgNHPm"
   },
   "source": [
    "The logits that come out of the model are of shape (batch_size, num_labels). As we are only forwarding a single sentence through the model, the `batch_size` equals 1. The logits is a tensor that contains the (unnormalized) scores for every individual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KOBosj4UL2tU",
    "outputId": "be370f49-3840-4c76-b193-76083e49701e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DC4XdDaHNVcd"
   },
   "source": [
    "To turn them into actual predicted labels, we first apply a sigmoid function independently to every score, such that every score is turned into a number between 0 and 1, that can be interpreted as a \"probability\" for how certain the model is that a given class belongs to the input text.\n",
    "\n",
    "Next, we use a threshold (typically, 0.5) to turn every probability into either a 1 (which means, we predict the label for the given example) or a 0 (which means, we don't predict the label for the given example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mEkAQleMMT0k",
    "outputId": "fddb51cb-bf01-420a-fd5b-4a7c2e775446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['side-effect']\n"
     ]
    }
   ],
   "source": [
    "# apply sigmoid + threshold\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(logits.squeeze().cpu())\n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= 0.5)] = 1\n",
    "# turn predicted id's into actual label names\n",
    "predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Fine-tuning BERT (and friends) for multi-label text classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "091f8220f33241f288faa0612853585f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6111a73e684a47769bda7183a836ee91",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8b1899a0c4b144d7a5e6599f8afb8b65",
      "value": 3
     }
    },
    "1045bb16e3694410898a73cf1b848917": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bbba60f793c14100934a268063f63d26",
      "placeholder": "​",
      "style": "IPY_MODEL_cd3570ddf67541d7818d97e236c54e54",
      "value": " 3/3 [00:00&lt;00:00, 75.93it/s]"
     }
    },
    "308fa6a7348140ec981a8d6c7d31f346": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5080d322a8034924b652b379c04667ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6111a73e684a47769bda7183a836ee91": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b1899a0c4b144d7a5e6599f8afb8b65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8bc92587e35443488445e7521fbd0a13": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5080d322a8034924b652b379c04667ed",
      "placeholder": "​",
      "style": "IPY_MODEL_cb95e545fbdd4e99903bf634df694c9f",
      "value": "100%"
     }
    },
    "9a1aa9f2cc29473f9f8e5459d2641e76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8bc92587e35443488445e7521fbd0a13",
       "IPY_MODEL_091f8220f33241f288faa0612853585f",
       "IPY_MODEL_1045bb16e3694410898a73cf1b848917"
      ],
      "layout": "IPY_MODEL_308fa6a7348140ec981a8d6c7d31f346"
     }
    },
    "bbba60f793c14100934a268063f63d26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb95e545fbdd4e99903bf634df694c9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd3570ddf67541d7818d97e236c54e54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
